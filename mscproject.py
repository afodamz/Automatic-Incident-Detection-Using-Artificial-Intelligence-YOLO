# -*- coding: utf-8 -*-

# Form implementation generated from reading ui file 'incident.ui'
#
# Created by: PyQt5 UI code generator 5.15.2
#
# WARNING: Any manual changes made to this file will be lost when pyuic5 is
# run again.  Do not edit this file unless you know what you are doing.

import os
# comment out below line to enable tensorflow outputs
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
import tensorflow as tf
physical_devices = tf.config.experimental.list_physical_devices('GPU')
if len(physical_devices) > 0:
    tf.config.experimental.set_memory_growth(physical_devices[0], True)
from absl import app, flags, logging
from absl.flags import FLAGS
import core.utils as utils
from core.yolov4 import filter_boxes
from core.functions import *
from tensorflow.python.saved_model import tag_constants
from PIL import Image
import cv2
import numpy as np
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession
import time
from datetime import datetime


from PyQt5 import QtCore, QtGui, QtWidgets



def main(_argv):
    flags.DEFINE_string('framework', 'tf', '(tf, tflite, trt')
    flags.DEFINE_string('weights', './checkpoints/incidents-416',
                        'path to weights file')
    flags.DEFINE_integer('size', 416, 'resize images to')
    flags.DEFINE_boolean('tiny', False, 'yolo or yolo-tiny')
    flags.DEFINE_string('model', 'yolov4', 'yolov3 or yolov4')
    flags.DEFINE_list('images', './data/images/', 'path to input image')
    flags.DEFINE_string('output', './detections/', 'path to output folder')
    flags.DEFINE_float('iou', 0.45, 'iou threshold')
    flags.DEFINE_float('score', 0.50, 'score threshold')
    flags.DEFINE_boolean('count', False, 'count objects within images')
    flags.DEFINE_boolean('dont_show', False, 'dont show image output')
    flags.DEFINE_boolean('info', False, 'print info on detections')
    flags.DEFINE_boolean('crop', False, 'crop detections from images')

    config = ConfigProto()
    config.gpu_options.allow_growth = True
    session = InteractiveSession(config=config)
    STRIDES, ANCHORS, NUM_CLASS, XYSCALE = utils.load_config(FLAGS)
    input_size = FLAGS.size
    images = FLAGS.images

    # load model
    if FLAGS.framework == 'tflite':
        interpreter = tf.lite.Interpreter(model_path=FLAGS.weights)
    else:
        saved_model_loaded = tf.saved_model.load(FLAGS.weights, tags=[tag_constants.SERVING])

    # loop through images in list and run Yolov4 model on each
    for count, image_path in enumerate(images, 1):
        original_image = cv2.imread(image_path)
        original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)

        image_data = cv2.resize(original_image, (input_size, input_size))
        image_data = image_data / 255.

        # get image name by using split method
        image_name = image_path.split('/')[-1]
        image_name = image_name.split('.')[0]

        images_data = []
        for i in range(1):
            images_data.append(image_data)
        images_data = np.asarray(images_data).astype(np.float32)

        now = datetime.now();
        if FLAGS.framework == 'tflite':
            interpreter.allocate_tensors()
            input_details = interpreter.get_input_details()
            output_details = interpreter.get_output_details()
            interpreter.set_tensor(input_details[0]['index'], images_data)
            interpreter.invoke()
            pred = [interpreter.get_tensor(output_details[i]['index']) for i in range(len(output_details))]
            if FLAGS.model == 'yolov3' and FLAGS.tiny == True:
                boxes, pred_conf = filter_boxes(pred[1], pred[0], score_threshold=0.25,
                                                input_shape=tf.constant([input_size, input_size]))
            else:
                boxes, pred_conf = filter_boxes(pred[0], pred[1], score_threshold=0.25,
                                                input_shape=tf.constant([input_size, input_size]))
        else:
            infer = saved_model_loaded.signatures['serving_default']
            batch_data = tf.constant(images_data)
            now = datetime.now();
            pred_bbox = infer(batch_data)
            for key, value in pred_bbox.items():
                boxes = value[:, :, 0:4]
                pred_conf = value[:, :, 4:]

        # run non max suppression on detections
        boxes, scores, classes, valid_detections = tf.image.combined_non_max_suppression(
            boxes=tf.reshape(boxes, (tf.shape(boxes)[0], -1, 1, 4)),
            scores=tf.reshape(
                pred_conf, (tf.shape(pred_conf)[0], -1, tf.shape(pred_conf)[-1])),
            max_output_size_per_class=50,
            max_total_size=50,
            iou_threshold=FLAGS.iou,
            score_threshold=FLAGS.score
        )

        # format bounding boxes from normalized ymin, xmin, ymax, xmax ---> xmin, ymin, xmax, ymax
        original_h, original_w, _ = original_image.shape
        bboxes = utils.format_boxes(boxes.numpy()[0], original_h, original_w)

        # hold all detection data in one variable
        
        pred_bbox = [bboxes, scores.numpy()[0], classes.numpy()[0], valid_detections.numpy()[0]]
        finish = datetime.now();
        print("time diff: ", now)
        print("time diff: ", finish)
        print("time diff: ", finish-now)

        # read in all class names from config
        class_names = utils.read_class_names(cfg.YOLO.CLASSES)

        # by default allow all classes in .names file
        allowed_classes = list(class_names.values())

        # custom allowed classes (uncomment line below to allow detections for only people)
        # allowed_classes = ['car', 'motorbike', 'bicycle']

        # if crop flag is enabled, crop each detection and save it as new image
        if FLAGS.crop:
            crop_path = os.path.join(os.getcwd(), 'detections', 'crop', image_name)
            try:
                os.mkdir(crop_path)
            except FileExistsError:
                pass
            crop_objects(cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB), pred_bbox, crop_path, allowed_classes)


        # if count flag is enabled, perform counting of objects
        if FLAGS.count:
            # count objects found
            counted_classes = count_objects(pred_bbox, by_class=True, allowed_classes=allowed_classes)
            # loop through dict and print
            for key, value in counted_classes.items():
                print("Number of {}s: {}".format(key, value))
            image = utils.draw_bbox(original_image, pred_bbox, FLAGS.info, counted_classes,
                                    allowed_classes=allowed_classes)
        else:
            image = utils.draw_bbox(original_image, pred_bbox, FLAGS.info, allowed_classes=allowed_classes)

        image = Image.fromarray(image.astype(np.uint8))
        if not FLAGS.dont_show:
            image.show()
        image = cv2.cvtColor(np.array(image), cv2.COLOR_BGR2RGB)
        cv2.imwrite(FLAGS.output + str(time.time()) + '.png', image)
        # cv2.imwrite(FLAGS.output + 'detection' + str(count) + '.png', image)


def runVideo(_argv):
    flags.DEFINE_string('framework', 'tf', '(tf, tflite, trt')
    flags.DEFINE_string('weights', './checkpoints/incidents-416',
                        'path to weights file')
    flags.DEFINE_integer('size', 416, 'resize images to')
    flags.DEFINE_boolean('tiny', False, 'yolo or yolo-tiny')
    flags.DEFINE_string('model', 'yolov4', 'yolov3 or yolov4')
    flags.DEFINE_string('video', './data/video/accidents.mp4', 'path to input video or set to 0 for webcam')
    flags.DEFINE_string('output', './detections/results.avi', 'path to output video')
    flags.DEFINE_string('output_format', 'XVID', 'codec used in VideoWriter when saving video to file')
    flags.DEFINE_float('iou', 0.45, 'iou threshold')
    flags.DEFINE_float('score', 0.50, 'score threshold')
    flags.DEFINE_boolean('count', False, 'count objects within video')
    flags.DEFINE_boolean('dont_show', False, 'dont show video output')
    flags.DEFINE_boolean('info', False, 'print info on detections')
    flags.DEFINE_boolean('crop', False, 'crop detections from images')

    config = ConfigProto()
    config.gpu_options.allow_growth = True
    session = InteractiveSession(config=config)
    STRIDES, ANCHORS, NUM_CLASS, XYSCALE = utils.load_config(FLAGS)
    input_size = FLAGS.size
    video_path = FLAGS.video
    # get video name by using split method
    video_name = video_path.split('/')[-1]
    video_name = video_name.split('.')[0]
    if FLAGS.framework == 'tflite':
        interpreter = tf.lite.Interpreter(model_path=FLAGS.weights)
        interpreter.allocate_tensors()
        input_details = interpreter.get_input_details()
        output_details = interpreter.get_output_details()
        print(input_details)
        print(output_details)
    else:
        saved_model_loaded = tf.saved_model.load(FLAGS.weights, tags=[tag_constants.SERVING])
        infer = saved_model_loaded.signatures['serving_default']

    # begin video capture
    try:
        vid = cv2.VideoCapture(int(video_path))
    except:
        vid = cv2.VideoCapture(video_path)

    out = None

    if FLAGS.output:
        # by default VideoCapture returns float instead of int
        width = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fps = int(vid.get(cv2.CAP_PROP_FPS))
        codec = cv2.VideoWriter_fourcc(*FLAGS.output_format)
        out = cv2.VideoWriter(FLAGS.output, codec, fps, (width, height))

    frame_num = 0
    while True:
        return_value, frame = vid.read()
        if return_value:
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frame_num += 1
            image = Image.fromarray(frame)
        else:
            print('Video has ended or failed, try a different video format!')
            break

        frame_size = frame.shape[:2]
        image_data = cv2.resize(frame, (input_size, input_size))
        image_data = image_data / 255.
        image_data = image_data[np.newaxis, ...].astype(np.float32)
        start_time = time.time()

        if FLAGS.framework == 'tflite':
            interpreter.set_tensor(input_details[0]['index'], image_data)
            interpreter.invoke()
            pred = [interpreter.get_tensor(output_details[i]['index']) for i in range(len(output_details))]
            if FLAGS.model == 'yolov3' and FLAGS.tiny == True:
                boxes, pred_conf = filter_boxes(pred[1], pred[0], score_threshold=0.25,
                                                input_shape=tf.constant([input_size, input_size]))
            else:
                boxes, pred_conf = filter_boxes(pred[0], pred[1], score_threshold=0.25,
                                                input_shape=tf.constant([input_size, input_size]))
        else:
            batch_data = tf.constant(image_data)
            pred_bbox = infer(batch_data)
            for key, value in pred_bbox.items():
                boxes = value[:, :, 0:4]
                pred_conf = value[:, :, 4:]

        boxes, scores, classes, valid_detections = tf.image.combined_non_max_suppression(
            boxes=tf.reshape(boxes, (tf.shape(boxes)[0], -1, 1, 4)),
            scores=tf.reshape(
                pred_conf, (tf.shape(pred_conf)[0], -1, tf.shape(pred_conf)[-1])),
            max_output_size_per_class=50,
            max_total_size=50,
            iou_threshold=FLAGS.iou,
            score_threshold=FLAGS.score
        )

        # format bounding boxes from normalized ymin, xmin, ymax, xmax ---> xmin, ymin, xmax, ymax
        original_h, original_w, _ = frame.shape
        bboxes = utils.format_boxes(boxes.numpy()[0], original_h, original_w)

        pred_bbox = [bboxes, scores.numpy()[0], classes.numpy()[0], valid_detections.numpy()[0]]

        # read in all class names from config
        class_names = utils.read_class_names(cfg.YOLO.CLASSES)

        # by default allow all classes in .names file
        allowed_classes = list(class_names.values())

        # custom allowed classes (uncomment line below to allow detections for only people)
        # allowed_classes = ['person']
        # allowed_classes = ['car', 'motorbike', 'bicycle']

        # if crop flag is enabled, crop each detection and save it as new image
        if FLAGS.crop:
            crop_rate = 150  # capture images every so many frames (ex. crop photos every 150 frames)
            crop_path = os.path.join(os.getcwd(), 'detections', 'crop', video_name)
            try:
                os.mkdir(crop_path)
            except FileExistsError:
                pass
            if frame_num % crop_rate == 0:
                final_path = os.path.join(crop_path, 'frame_' + str(frame_num))
                try:
                    os.mkdir(final_path)
                except FileExistsError:
                    pass
                crop_objects(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB), pred_bbox, final_path, allowed_classes)
            else:
                pass

        if FLAGS.count:
            # count objects found
            counted_classes = count_objects(pred_bbox, by_class=True, allowed_classes=allowed_classes)
            # loop through dict and print
            for key, value in counted_classes.items():
                print("Number of {}s: {}".format(key, value))
            image = utils.draw_bbox(frame, pred_bbox, FLAGS.info, counted_classes, allowed_classes=allowed_classes)
        else:
            image = utils.draw_bbox(frame, pred_bbox, FLAGS.info, allowed_classes=allowed_classes)

        fps = 1.0 / (time.time() - start_time)
        print("FPS: %.2f" % fps)
        result = np.asarray(image)
        cv2.namedWindow("result", cv2.WINDOW_AUTOSIZE)
        result = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)

        if not FLAGS.dont_show:
            cv2.imshow("result", result)

        if FLAGS.output:
            out.write(result)
        if cv2.waitKey(1) & 0xFF == ord('q'): break
    cv2.destroyAllWindows()




class Ui_IncidentDetection(object):
    def setupUi(self, IncidentDetection):
        IncidentDetection.setObjectName("IncidentDetection")
        IncidentDetection.resize(689, 555)
        IncidentDetection.setMouseTracking(False)
        icon = QtGui.QIcon()
        icon.addPixmap(QtGui.QPixmap("../../afodamz documents/doc Pics/Afolabi-Oluwadamilola.jpg"), QtGui.QIcon.Normal, QtGui.QIcon.Off)
        IncidentDetection.setWindowIcon(icon)
        self.centralwidget = QtWidgets.QWidget(IncidentDetection)
        self.centralwidget.setMinimumSize(QtCore.QSize(689, 511))
        self.centralwidget.setMaximumSize(QtCore.QSize(689, 511))
        self.centralwidget.setObjectName("centralwidget")
        self.verticalLayout_2 = QtWidgets.QVBoxLayout(self.centralwidget)
        self.verticalLayout_2.setObjectName("verticalLayout_2")
        self.horizontalWidget = QtWidgets.QWidget(self.centralwidget)
        self.horizontalWidget.setMinimumSize(QtCore.QSize(671, 493))
        self.horizontalWidget.setMaximumSize(QtCore.QSize(671, 493))
        self.horizontalWidget.setStyleSheet("QFrame{\n"
"border:1px solid black;\n"
"border-radius: 20px;\n"
"padding: 10px;\n"
"}\n"
"QLabel{\n"
"border:none;\n"
"}\n"
"QPushButton {\n"
"    padding:5px;\n"
"     background-color: rgb(252, 233, 79);\n"
# "transition: 0.9s; \n"
" }\n"
" QPushButton:hover {\n"
"     background-color: rgb(224, 255, 0);\n"
" }\n"
" QPushButton:hover:!pressed{\n"
"     background-color: rgb(224, 0, 0);     \n"
" }\n"
"")
        self.horizontalWidget.setObjectName("horizontalWidget")
        self.gridLayout = QtWidgets.QGridLayout(self.horizontalWidget)
        self.gridLayout.setObjectName("gridLayout")
        self.imageincident = QtWidgets.QFrame(self.horizontalWidget)
        self.imageincident.setAutoFillBackground(False)
        self.imageincident.setStyleSheet("")
        self.imageincident.setObjectName("imageincident")
        self.gridLayout_2 = QtWidgets.QGridLayout(self.imageincident)
        self.gridLayout_2.setContentsMargins(0, 0, 0, 0)
        self.gridLayout_2.setObjectName("gridLayout_2")
        self.incident_button = QtWidgets.QPushButton(self.imageincident)
        self.incident_button.setMinimumSize(QtCore.QSize(80, 40))
        self.incident_button.setMaximumSize(QtCore.QSize(302, 40))
        font = QtGui.QFont()
        font.setKerning(True)
        self.incident_button.setFont(font)
        self.incident_button.setCursor(QtGui.QCursor(QtCore.Qt.PointingHandCursor))
        self.incident_button.setMouseTracking(True)
        self.incident_button.setShortcut("")
        self.incident_button.setAutoRepeat(False)
        self.incident_button.setAutoDefault(False)
        self.incident_button.setObjectName("incident_button")
        self.incident_button.clicked.connect(self.ImageIncident)
        self.gridLayout_2.addWidget(self.incident_button, 1, 0, 1, 1, QtCore.Qt.AlignVCenter)
        self.incident_title = QtWidgets.QLabel(self.imageincident)
        self.incident_title.setAutoFillBackground(False)
        self.incident_title.setFrameShape(QtWidgets.QFrame.StyledPanel)
        self.incident_title.setFrameShadow(QtWidgets.QFrame.Raised)
        self.incident_title.setLineWidth(13)
        self.incident_title.setMidLineWidth(0)
        self.incident_title.setAlignment(QtCore.Qt.AlignCenter)
        self.incident_title.setObjectName("incident_title")
        self.gridLayout_2.addWidget(self.incident_title, 0, 0, 1, 1)
        self.gridLayout.addWidget(self.imageincident, 3, 0, 1, 1)
        self.main_label = QtWidgets.QLabel(self.horizontalWidget)
        font = QtGui.QFont()
        font.setPointSize(20)
        self.main_label.setFont(font)
        self.main_label.setStyleSheet("")
        self.main_label.setTextFormat(QtCore.Qt.RichText)
        self.main_label.setAlignment(QtCore.Qt.AlignCenter)
        self.main_label.setWordWrap(False)
        self.main_label.setObjectName("main_label")
        self.gridLayout.addWidget(self.main_label, 1, 0, 1, 2)
        self.videoincident = QtWidgets.QFrame(self.horizontalWidget)
        self.videoincident.setObjectName("videoincident")
        self.gridLayout_3 = QtWidgets.QGridLayout(self.videoincident)
        self.gridLayout_3.setObjectName("gridLayout_3")
        self.incident_title_2 = QtWidgets.QLabel(self.videoincident)
        self.incident_title_2.setAlignment(QtCore.Qt.AlignCenter)
        self.incident_title_2.setObjectName("incident_title_2")
        self.gridLayout_3.addWidget(self.incident_title_2, 0, 0, 1, 1)
        self.videobutton = QtWidgets.QPushButton(self.videoincident)
        self.videobutton.setMinimumSize(QtCore.QSize(80, 40))
        self.videobutton.setMaximumSize(QtCore.QSize(283, 40))
        self.videobutton.setObjectName("videobutton")
        self.videobutton.clicked.connect(self.VideoIncident)
        self.gridLayout_3.addWidget(self.videobutton, 1, 0, 1, 1, QtCore.Qt.AlignVCenter)
        self.gridLayout.addWidget(self.videoincident, 3, 1, 1, 1)
        self.gridFrame = QtWidgets.QFrame(self.horizontalWidget)
        self.gridFrame.setObjectName("gridFrame")
        self.webcamincident = QtWidgets.QGridLayout(self.gridFrame)
        self.webcamincident.setObjectName("webcamincident")
        self.label = QtWidgets.QLabel(self.gridFrame)
        font = QtGui.QFont()
        font.setBold(False)
        font.setWeight(50)
        self.label.setFont(font)
        self.label.setAlignment(QtCore.Qt.AlignCenter)
        self.label.setObjectName("label")
        self.webcamincident.addWidget(self.label, 0, 0, 1, 1)
        self.webcambutton = QtWidgets.QPushButton(self.gridFrame)
        self.webcambutton.setMinimumSize(QtCore.QSize(80, 40))
        self.webcambutton.setMaximumSize(QtCore.QSize(300, 50))
        self.webcambutton.setObjectName("webcambutton")
        self.webcamincident.addWidget(self.webcambutton, 1, 0, 1, 1)
        self.gridLayout.addWidget(self.gridFrame, 4, 0, 1, 1)
        self.verticalLayout_2.addWidget(self.horizontalWidget)
        IncidentDetection.setCentralWidget(self.centralwidget)
        self.menubar = QtWidgets.QMenuBar(IncidentDetection)
        self.menubar.setGeometry(QtCore.QRect(0, 0, 689, 22))
        self.menubar.setObjectName("menubar")
        self.menuAbout = QtWidgets.QMenu(self.menubar)
        self.menuAbout.setObjectName("menuAbout")
        self.menuHelp = QtWidgets.QMenu(self.menubar)
        self.menuHelp.setObjectName("menuHelp")
        self.menuFile = QtWidgets.QMenu(self.menubar)
        self.menuFile.setObjectName("menuFile")
        IncidentDetection.setMenuBar(self.menubar)
        self.statusbar = QtWidgets.QStatusBar(IncidentDetection)
        self.statusbar.setObjectName("statusbar")
        IncidentDetection.setStatusBar(self.statusbar)
        self.actionNew = QtWidgets.QAction(IncidentDetection)
        self.actionNew.setObjectName("actionNew")
        self.actionEdit = QtWidgets.QAction(IncidentDetection)
        self.actionEdit.setObjectName("actionEdit")
        self.actionExit = QtWidgets.QAction(IncidentDetection)
        self.actionExit.setObjectName("actionExit")
        self.actionExit.triggered.connect(self.closeEvent)
        self.menuFile.addAction(self.actionNew)
        self.menuFile.addAction(self.actionEdit)
        self.menuFile.addSeparator()
        self.menuFile.addAction(self.actionExit)
        self.menubar.addAction(self.menuFile.menuAction())
        self.menubar.addAction(self.menuAbout.menuAction())
        self.menubar.addAction(self.menuHelp.menuAction())

        self.retranslateUi(IncidentDetection)
        QtCore.QMetaObject.connectSlotsByName(IncidentDetection)

    def retranslateUi(self, IncidentDetection):
        _translate = QtCore.QCoreApplication.translate
        IncidentDetection.setWindowTitle(_translate("IncidentDetection", "Incident Detection"))
        self.incident_button.setText(_translate("IncidentDetection", "Get Image"))
        self.incident_title.setText(_translate("IncidentDetection", "IMAGE INCIDENT DETECTION"))
        self.main_label.setText(_translate("IncidentDetection", "<html><head/><body><p><span style=\" font-weight:600;\">INCIDENT DETECTION</span></p></body></html>"))
        self.incident_title_2.setText(_translate("IncidentDetection", "VIDEO INCIDENT DETECTION"))
        self.videobutton.setText(_translate("IncidentDetection", "Get Video"))
        self.label.setText(_translate("IncidentDetection", "CAM INCIDENT"))
        self.webcambutton.setText(_translate("IncidentDetection", "Set WebCam"))
        self.menuAbout.setTitle(_translate("IncidentDetection", "About"))
        self.menuHelp.setTitle(_translate("IncidentDetection", "Help"))
        self.menuFile.setTitle(_translate("IncidentDetection", "File"))
        self.actionNew.setText(_translate("IncidentDetection", "New"))
        self.actionEdit.setText(_translate("IncidentDetection", "Edit"))
        self.actionExit.setText(_translate("IncidentDetection", "Exit"))

    def ImageIncident(self):
        import sys
        print(sys.argv)
        script_descriptor = open("detect.py")
        a_script = script_descriptor.read()
        # sys.argv = ["a_script.py", "arg1", "arg2", "arg3"]
        print("""working........""")
        exec(a_script)

    def ImageIncident(self):
        try:
            app.run(main)
        except SystemExit:
            pass

    def VideoIncident(self):
        try:
            app.run(runVideo)
        except SystemExit:
            pass

    def closeEvent(self, event):
        import sys
        close = QtWidgets.QMessageBox()
        close.setText("Are You sure you want to exit?")
        close.setStandardButtons(QtWidgets.QMessageBox.Yes | QtWidgets.QMessageBox.Cancel)
        close = close.exec()

        if close == QtWidgets.QMessageBox.Yes:
            sys.exit()
            # event.accept()
        else:
            pass
            # event.ignore()

if __name__ == "__main__":
    import sys
    incidentapp = QtWidgets.QApplication(sys.argv)
    IncidentDetection = QtWidgets.QMainWindow()
    ui = Ui_IncidentDetection()
    ui.setupUi(IncidentDetection)
    IncidentDetection.show()
    sys.exit(incidentapp.exec_())

